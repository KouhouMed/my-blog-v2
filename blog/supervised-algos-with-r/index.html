<!doctype html><html lang=en-us class="m-auto dark"><head><title>Mohamed Kouhou's personal website</title>
<meta name=theme-color content><meta charset=utf-8><meta content="width=device-width,initial-scale=1" name=viewport><meta name=description content="Website title"><meta name=author content="Mohamed Kouhou"><meta name=generator content="aafu theme by Darshan in Hugo 0.126.0"><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/safari-pinned-tab.svg color=#252627><link rel="shortcut icon" href=/favicon.ico><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.15.2/css/all.css integrity=sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu crossorigin=anonymous><link rel=stylesheet href=https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css><link rel=stylesheet href="//fonts.googleapis.com/css?family=Didact+Gothic%7CRoboto:400%7CRoboto+Mono"><link href="/main.min.eaa78b078d6eb351a68a203ecef114c1aa4eaa6686741d9320c9c8f5e6a4099c.css" rel=stylesheet><script>let html=document.querySelector("html"),theme=window.localStorage.getItem("theme");theme?theme==="dark"?html.classList.add("dark"):html.classList.remove("dark"):html.classList.contains("dark")?window.localStorage.setItem("theme","dark"):(html.classList.remove("dark"),window.localStorage.setItem("theme","light")),window.onload=()=>{let e=document.querySelector(".theme-toggle");window.localStorage.getItem("theme")==="dark"?(e.classList.remove("bi-moon-fill"),e.classList.add("bi-brightness-high")):(e.classList.add("bi-moon-fill"),e.classList.remove("bi-brightness-high"));let t=document.querySelector(".accordion.active");t&&(t.nextElementSibling.style.maxHeight=t.nextElementSibling.scrollHeight+"px")},window.onresize=()=>{let e=document.querySelector(".accordion.active");e&&(e.nextElementSibling.style.maxHeight=e.nextElementSibling.scrollHeight+"px")}</script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:["script","noscript","style","textarea","pre"]}},window.addEventListener("load",e=>{document.querySelectorAll("mjx-container").forEach(function(e){e.parentElement.classList+="has-jax"})})</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></head><body class="h-screen p-2 m-auto max-w-4xl flex flex-col"><header class="nav flex flex-row row py-2 mb-6 w-full border-b border-gray-300 dark:border-gray-700 justify-between"><div><a class="no-underline p-2 rounded hover:bg-gray-200 dark:hover:bg-gray-800" href=https://kouhoumed.com/>Home</a>
<a class="no-underline p-2 rounded hover:bg-gray-200 dark:hover:bg-gray-800" href=/blog>Blog</a></div><div><a class=mr-4 href=/search><i class="fas fa-search"></i></i></a>
<i class="fas fa-sun theme-toggle text-blue-500 hover:text-blue-700 dark:text-yellow-300 dark:hover:text-yellow-500 cursor-pointer text-lg mr-9 sm:mr-0" onclick=lightDark(this)></i></div></header><script>const lightDark=e=>{let t=document.querySelector("html");t.classList.contains("dark")?(t.classList.remove("dark"),e.classList.add("fa-moon"),e.classList.remove("fa-sun"),window.localStorage.setItem("theme","light")):(t.classList.add("dark"),e.classList.add("fa-sun"),e.classList.remove("fa-moon"),window.localStorage.setItem("theme","dark"))}</script><main class=grow><div class="prose prose-stone dark:prose-invert max-w-none"><div class=mb-3><h1 class=top-h1 style=font-size:2.75em>Supervised Learning algorithms with R (part 1)</h1><p class=mb-1>January 5, 2021</p><p>&mdash;</p></div><div class=content><p>As I mentioned in an earlier <a href=https://kouhoumed.site/blog/ml/>post</a>, machine learning algorithms are categorized into three main types :</p><ul><li>Supervised learning algorithms</li><li>Unsupervised learning algorithms</li><li>Reinfocement learning algorithms</li></ul><p>In this article, we will only talk about some of machine learning supervised algorithms. We&rsquo;ll also see examples using R programming language.</p><p>First of all, I would like to remind you that supervised learning is a machine learning algorithm that tries to find a function mapping an input to a given output based on a set of examples. Basically, each of these examples (called <strong>training set</strong>) consists of the input $X$ and the desired output $Y$. In other words, we try to approximate the function $f$ such that $f(X)=Y$ using previously labelled data as learning examples. The performance of such a model is evaluated upon its ability to generalize onto new data that is unlabeled.</p><h2 id=k-nearest-neighbours>K-Nearest Neighbours</h2><p>K-Nearest Neighbours (KNN) is a simple Machine Learning algorithm based on Supervised Learning technique. It can be used for both regression and classification, but it is mostly used in classification.</p><p>Suppose we have two categories : Category $A$ and Category $B$, and we have a new data point $x$ that we want to know to which category it belongs.</p><center><img src=https://miro.medium.com/max/800/1*2zYNhLc522h0zftD1zDh2g.png style=width:50%;height:auto></center><p>KNN algorithm is comprised of the following steps :</p><ul><li><strong>Step 1 :</strong> Select K, number of neighbours</li><li><strong>Step 2 :</strong> Calculate the distance of between $x$ and each of the other data points (we can use the Euclidian distance or others)</li><li><strong>Step 3 :</strong> Take the K nearest neighbors as per the calculated Euclidean distance.</li><li><strong>Step 4 :</strong> Among these k neighbors, count the number of the data points in each category.</li><li><strong>Step 5 :</strong> Assign the new data points to that category for which the number of the neighbor is maximum.</li></ul><p><strong>NB 1 :</strong> The Euclidian distance between two points $a_1(x_1,y_1)$ and $a_2(x_2,y_2)$ is calculated as follows : $$d(a_1,a_2)=\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$$</p><center><img src=https://i.ibb.co/DrGLq7q/unnamed.png style=width:50%;height:auto></center>A more general formula in an n-dimensional space is : $$d(x,y)=\sqrt{\sum_{i=1}^{n}(x_i-y_i)^2}$$ where $x(x_1,...,x_n)$ and $y(y_1,...,y_n)$.<p><strong>NB 2 :</strong> As can be seen, there are no parameters that need to be learned during training to determine whether a new observation belongs to class ùê¥ or ùêµ. The only parameter used in K-Nearest Neighbours is K, which is a predetermined value. The algorithm simply works by looking at the training samples, calculating distances and finding the K examples in the training set that are closest to the new observation. Thus, KNN is a <strong>non-parametric</strong>, <strong>supervised</strong> (needs training labels) learning algorithm.</p><p><strong>NB 3 :</strong> KNN does support <a href=https://en.wikipedia.org/wiki/Categorical_variable>categorical variables</a> as features, simply because we cannot calculated the distance from them.</p><p>The hands-on example<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> that we will work on will use the <code>Sonar</code> data set (signals) from <code>mlbench</code> library. <code>Sonar</code> is a system for the detection of objects under water and for measuring the water&rsquo;s depth by emitting and detecting sound pulses (the complete description can be found ‚Üí<a href=https://cran.r-project.org/web/packages/mlbench/mlbench.pdf>here</a>). For our purposes, this is a two-class (class $R$ and class $M$) classification task with numeric data.</p><p>First of all, let&rsquo;s install the required libraries :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#75715e># install the packages (note: this may take some time)</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>install.packages</span>(<span style=color:#e6db74>&#34;class&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#a6e22e>install.packages</span>(<span style=color:#e6db74>&#34;caret&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#a6e22e>install.packages</span>(<span style=color:#e6db74>&#34;mlbench&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#a6e22e>install.packages</span>(<span style=color:#e6db74>&#34;e1071&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>library</span>(class)
</span></span><span style=display:flex><span><span style=color:#a6e22e>library</span>(caret)
</span></span><span style=display:flex><span><span style=color:#a6e22e>require</span>(mlbench)
</span></span><span style=display:flex><span><span style=color:#a6e22e>library</span>(e1071)
</span></span><span style=display:flex><span><span style=color:#a6e22e>library</span>(base)
</span></span><span style=display:flex><span><span style=color:#a6e22e>require</span>(base)
</span></span></code></pre></div><h4 id=step-1--loading-the-data>Step 1 : Loading the data</h4><p>Let&rsquo;s load the <code>Sonar</code> dataset and look at the first five rows :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>data</span>(Sonar)
</span></span><span style=display:flex><span><span style=color:#a6e22e>head</span>(Sonar)
</span></span></code></pre></div><h4 id=step-2--preparing-and-exploring-the-data>Step 2 : Preparing and exploring the data</h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>nrow</span>(Sonar)
</span></span><span style=display:flex><span><span style=color:#a6e22e>ncol</span>(Sonar)
</span></span></code></pre></div><p>This will display the number of lines (<em><strong>208 observations</strong></em>) and the number of columns (<em><strong>61 variables</strong></em>), all numerical except for the Class variable which is categorical.</p><p>Let&rsquo;s check how many $R$ classes and $M$ classes <code>Sonar</code> contains :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>base<span style=color:#f92672>::</span><span style=color:#a6e22e>table</span>(Sonar<span style=color:#f92672>$</span>Class)
</span></span></code></pre></div><p>Now let&rsquo;s see if it contains any <code>NA</code> values in its columns :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>apply</span>(Sonar, <span style=color:#ae81ff>2</span>, <span style=color:#66d9ef>function</span>(x) <span style=color:#a6e22e>sum</span>(<span style=color:#a6e22e>is.na</span>(x))) 
</span></span></code></pre></div><p>We are going to manually split <code>Sonar</code> into training and test sets. Here, we will dedicate 70% of the dataset for traing, and the rest for testing :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>SEED <span style=color:#f92672>&lt;-</span> <span style=color:#ae81ff>123</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>set.seed</span>(SEED)
</span></span><span style=display:flex><span>data <span style=color:#f92672>&lt;-</span> Sonar[base<span style=color:#f92672>::</span><span style=color:#a6e22e>sample</span>(<span style=color:#a6e22e>nrow</span>(Sonar)), ] <span style=color:#75715e># shuffle data first</span>
</span></span><span style=display:flex><span>bound <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>floor</span>(<span style=color:#ae81ff>0.7</span> <span style=color:#f92672>*</span> <span style=color:#a6e22e>nrow</span>(data))
</span></span><span style=display:flex><span>df_train <span style=color:#f92672>&lt;-</span> data[1<span style=color:#f92672>:</span>bound, ] 
</span></span><span style=display:flex><span>df_test <span style=color:#f92672>&lt;-</span> data<span style=color:#a6e22e>[</span>(bound <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)<span style=color:#f92672>:</span><span style=color:#a6e22e>nrow</span>(data), ]
</span></span><span style=display:flex><span><span style=color:#a6e22e>cat</span>(<span style=color:#e6db74>&#34;Number of training and test samples are &#34;</span>, <span style=color:#a6e22e>nrow</span>(df_train), <span style=color:#a6e22e>nrow</span>(df_test))
</span></span></code></pre></div><p>Now, let&rsquo;s create the following dataframes :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>X_train <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>subset</span>(df_train, select<span style=color:#f92672>=-</span>Class)
</span></span><span style=display:flex><span>y_train <span style=color:#f92672>&lt;-</span> df_train<span style=color:#f92672>$</span>Class
</span></span><span style=display:flex><span>X_test <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>subset</span>(df_test, select<span style=color:#f92672>=-</span>Class) <span style=color:#75715e># exclude Class for prediction</span>
</span></span><span style=display:flex><span>y_test <span style=color:#f92672>&lt;-</span> df_test<span style=color:#f92672>$</span>Class
</span></span></code></pre></div><h4 id=tep-3--training-a-model-on-data>tep 3 : Training a model on data</h4><p>Now, we are going to use <code>knn</code> function from <code>class</code> library with $K=3$ :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>knn_model <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>knn</span>(train<span style=color:#f92672>=</span>X_train,
</span></span><span style=display:flex><span>                 test<span style=color:#f92672>=</span>X_test,
</span></span><span style=display:flex><span>                 cl<span style=color:#f92672>=</span>y_train,  <span style=color:#75715e># class labels</span>
</span></span><span style=display:flex><span>                 k<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>knn_model
</span></span></code></pre></div><p>If you run the code above, you&rsquo;ll see the prediction made by <code>knn_model</code> with $K=3$ on <code>X_test</code>.</p><h4 id=step-4--evaluate-the-model-performance>Step 4 : Evaluate the model performance</h4><p>In order to see how many classes have been correctly or incorrectly classified, we can create a <strong><a href=https://en.wikipedia.org/wiki/Confusion_matrix>confusion matrix</a></strong> as follows :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>conf_mat <span style=color:#f92672>&lt;-</span> base<span style=color:#f92672>::</span><span style=color:#a6e22e>table</span>(y_test, knn_model)
</span></span><span style=display:flex><span>conf_mat
</span></span></code></pre></div><p>To compute the <strong><a href=https://en.wikipedia.org/wiki/Accuracy_and_precision>accuracy</a></strong>, we sum up all the correctly classified observations (located in diagonal) and divide it by the total number of classes :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>cat</span>(<span style=color:#e6db74>&#34;Accuracy: &#34;</span>, <span style=color:#a6e22e>sum</span>(<span style=color:#a6e22e>diag</span>(conf_mat))<span style=color:#f92672>/</span><span style=color:#a6e22e>sum</span>(conf_mat))
</span></span></code></pre></div><p>To assess whether $K=3$ is a good choice and see whether $K=3$ leads to <a href=https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/>overfitting/underfitting</a> the data, we could use <code>knn.cv</code> which does the <strong>leave-one-out cross-validations</strong> for training set (i.e., it singles out a training sample one at a time and tries to view it as a new example and see what class label it assigns).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>knn_loocv <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>knn.cv</span>(train<span style=color:#f92672>=</span>X_train, cl<span style=color:#f92672>=</span>y_train, k<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>knn_loocv
</span></span></code></pre></div><p>Let&rsquo;s create a confusion matrix to compute the accuracy of the training labels <code>y_train</code> and the cross-validated predictions <code>knn_loocv</code>, same as the above :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>conf_mat_cv <span style=color:#f92672>&lt;-</span> base<span style=color:#f92672>::</span><span style=color:#a6e22e>table</span>(y_train, knn_loocv)
</span></span><span style=display:flex><span>conf_mat_cv
</span></span><span style=display:flex><span><span style=color:#a6e22e>cat</span>(<span style=color:#e6db74>&#34;LOOCV accuracy: &#34;</span>, <span style=color:#a6e22e>sum</span>(<span style=color:#a6e22e>diag</span>(conf_mat_cv)) <span style=color:#f92672>/</span> <span style=color:#a6e22e>sum</span>(conf_mat_cv))
</span></span></code></pre></div><p>The difference between the cross-validated accuracy and the test accuracy shows that $K=3$ leads to overfitting. Perhaps we should change $K$ to lessen the overfitting.</p><h4 id=step-5--improve-the-performance-of-the-model>Step 5 : Improve the performance of the model</h4><p>There are a couple things we can do in order to improve the performance of our model :</p><ul><li><strong>Centering and scaling data</strong> : these are forms of preprocessing numerical data (not suitable for categorical data). Centering a variable means subtracting the mean of the variable from each data point so that the new variable&rsquo;s mean is 0. And scaling consists of multiplying each data point by a constant in order to alter the range of the data.</li><li><strong>Performing a <em>cross-vaidation</em></strong> : this consists of dividing the data into a finite number of subsets. Through each iteration, a subset is set aside, and the remaining subsets are used as the training set. The subset that was set aside is used as the test set (prediction). We will use <code>caret</code> library for this purpose.</li></ul><p>This is a method of cross-referencing the model built using its own data :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>SEED <span style=color:#f92672>&lt;-</span> <span style=color:#ae81ff>2016</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>set.seed</span>(SEED)
</span></span><span style=display:flex><span><span style=color:#75715e># create the training data 70% of the overall Sonar data.</span>
</span></span><span style=display:flex><span>in_train <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>createDataPartition</span>(Sonar<span style=color:#f92672>$</span>Class, p<span style=color:#f92672>=</span><span style=color:#ae81ff>0.7</span>, list<span style=color:#f92672>=</span><span style=color:#66d9ef>FALSE</span>) <span style=color:#75715e># create training indices</span>
</span></span><span style=display:flex><span>ndf_train <span style=color:#f92672>&lt;-</span> Sonar[in_train, ]
</span></span><span style=display:flex><span>ndf_test <span style=color:#f92672>&lt;-</span> Sonar[<span style=color:#f92672>-</span>in_train, ]
</span></span></code></pre></div><p>Here, we specify the cross-validation method we want to use to find the best $K$ in grid search.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#75715e># lets create a function setup to do 5-fold cross-validation with 2 repeat.</span>
</span></span><span style=display:flex><span>ctrl <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>trainControl</span>(method<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;repeatedcv&#34;</span>, number<span style=color:#f92672>=</span><span style=color:#ae81ff>5</span>, repeats<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>nn_grid <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>expand.grid</span>(k<span style=color:#f92672>=</span><span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>3</span>,<span style=color:#ae81ff>5</span>,<span style=color:#ae81ff>7</span>))
</span></span><span style=display:flex><span>nn_grid
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>set.seed</span>(SEED)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>best_knn <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>train</span>(Class<span style=color:#f92672>~</span>., data<span style=color:#f92672>=</span>ndf_train,
</span></span><span style=display:flex><span>                  method<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;knn&#34;</span>,
</span></span><span style=display:flex><span>                  trControl<span style=color:#f92672>=</span>ctrl, 
</span></span><span style=display:flex><span>                  preProcess <span style=color:#f92672>=</span> <span style=color:#a6e22e>c</span>(<span style=color:#e6db74>&#34;center&#34;</span>, <span style=color:#e6db74>&#34;scale&#34;</span>),  <span style=color:#75715e># standardize</span>
</span></span><span style=display:flex><span>                  tuneGrid<span style=color:#f92672>=</span>nn_grid)
</span></span><span style=display:flex><span>best_knn
</span></span></code></pre></div><p>Running the code above, you&rsquo;ll find out that $K=1$ has the highest accuracy from repeated cross-validation.</p><h2 id=decision-trees>Decision Trees</h2><p><strong>Decision Trees</strong> are one of the most powerful predictive classification models. They are based on the analysis of a set of data points that describe the type of object we want to classify. A Decision tree is a flowchart like tree structure, where each <strong>internal node</strong> denotes a test on an attribute, each <strong>branch</strong> represents an outcome of the test, and each <strong>leaf node</strong> (terminal node) holds a class label.</p><center><img src=https://upload.wikimedia.org/wikipedia/commons/6/66/Champignons_mushrooms_%28950475736%29.jpg style=width:50%;height:auto></center><p>In our practical example<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>, we&rsquo;ll try to classify a set of mushrooms as either <em>edible</em> or <em>poisonous</em> based on features like its cap type, color, odor, shape of its stalk, etc.</p><center><figure><img src=https://ibm.box.com/shared/static/ar8rlcoyrs0n0kphj4g4n4rbhe76vpd9.png style=width:80%;height:auto><figcaption>Example of mushroom features and their classification</figcaption></figure></center><p>The algorithm behind Decision trees uses probabilities. For example, if many mushrooms that have large caps are poisonous, the algorithm will assume that the probability of large-cap mushrooms being poisonous is high. When the model is complete, we have a tree-like structure composed of what are called <strong>decision nodes</strong>, which ask our data point questions about its features, and <strong>leaf nodes</strong>, which tells us what classification the decision tree thinks our data point is.</p><center><figure><img src=https://ibm.box.com/shared/static/urnm2onpitt8qz2296mltzcfdn1p040f.png style=width:80%;height:auto><figcaption>Example of a possible Decision Tree describing mushrooms</figaption></figure></center><p>The goal of a decision tree is to split the dataset on based on attributes. But how to find the best feature in each node to split?</p><p>To answer this question, let&rsquo;s first define the <strong>Entropy</strong>.</p><p><strong>Entropy</strong> is the amount of information disorder, or the amount of randomness in the data. It is calculated for each node and it depends on how much random data that node contains. In decision tree we are looking for a trees that have smallest entropy in their nodes. The entropy is used to calculate the homogeneity of the samples in that node. If the samples are completely homogeneous the entropy is zero and if the sample is an equally divided it has entropy of one. It means, if all data in a node are either poisonous or edible, then the entropy is zero, but if the half of data are poisonous and other half are edible, then the entropuy is one. In our example, we can calculate the Entropy of our target class using the following formula :</p><p>$$Entropy = - p(edible)log(p(edible)) - p(poisonous)log(p(poisonous))$$</p><p>Decision trees use another metric on which decisions are based : <strong>Information gain</strong>. We can think of it as the opposite of entropy. The more randomness decreases, the more information we gain, and vice-versa. Thus, while building a decision tree, we choose the attributes with the highest information gain.
$$\text{Information Gain = entropy(parent) ‚Äì [average entropy(children)]}$$</p><p><strong>Algorithm :</strong></p><pre><code>  1. Calculate entropy of the target field (the class label) for whole dataset.
  2. For each attribute:
    - split the dataset on the attribute
    - calculate entropy of the target field on splited dataset, using the attribute values
    - calculate the information gain of the attribute
  3. select the attribute that has the largest informmation gain
  4. Branch the tree using the selected attribute
  5. stop, if it is a node with entropy of 0, otherwise jump to step2
</code></pre><h4 id=decision-tree-with-r>Decision tree with R</h4><p>We will start by loading the data. We&rsquo;ll use <a href=https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/>UCI&rsquo;s</a> <code>Mushroom</code> dataset. Since this dataset is not inbuilt into R, we need to download it and load it into R :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>download.file</span>(<span style=color:#e6db74>&#34;https://ibm.box.com/shared/static/dpdh09s70abyiwxguehqvcq3dn0m7wve.data&#34;</span>, <span style=color:#e6db74>&#34;mushroom.data&#34;</span>)
</span></span></code></pre></div><p>After downloading the file, we need to create a data frame to house the observations in the dataset. Since the dataset is structured using comma-separated values, we can use the <code>read.csv</code> function :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>mushrooms <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>read.csv</span>(<span style=color:#e6db74>&#34;mushroom.data&#34;</span>, header <span style=color:#f92672>=</span> F)
</span></span><span style=display:flex><span>mushrooms
</span></span></code></pre></div><p>Once that&rsquo;s done, we have the data loaded up. However, the way that it is structured isn&rsquo;t the most intuitive. In the code cell below, we are adding the column names to the data frame with the <code>colnames</code> function. Additionally, since our data frame is composed of factors, we can rename some of these factors to something more easily understood by us using <code>levels</code>.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#75715e># Define column names for the mushrooms data frame.</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>colnames</span>(mushrooms) <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>c</span>(<span style=color:#e6db74>&#34;Class&#34;</span>,<span style=color:#e6db74>&#34;cap.shape&#34;</span>,<span style=color:#e6db74>&#34;cap.surface&#34;</span>,<span style=color:#e6db74>&#34;cap.color&#34;</span>,<span style=color:#e6db74>&#34;bruises&#34;</span>,<span style=color:#e6db74>&#34;odor&#34;</span>,<span style=color:#e6db74>&#34;gill.attachment&#34;</span>,<span style=color:#e6db74>&#34;gill.spacing&#34;</span>,
</span></span><span style=display:flex><span>                         <span style=color:#e6db74>&#34;gill.size&#34;</span>,<span style=color:#e6db74>&#34;gill.color&#34;</span>,<span style=color:#e6db74>&#34;stalk.shape&#34;</span>,<span style=color:#e6db74>&#34;stalk.root&#34;</span>,<span style=color:#e6db74>&#34;stalk.surface.above.ring&#34;</span>,
</span></span><span style=display:flex><span>                         <span style=color:#e6db74>&#34;stalk.surface.below.ring&#34;</span>,<span style=color:#e6db74>&#34;stalk.color.above.ring&#34;</span>,<span style=color:#e6db74>&#34;stalk.color.below.ring&#34;</span>,<span style=color:#e6db74>&#34;veil.type&#34;</span>,<span style=color:#e6db74>&#34;veil.color&#34;</span>,
</span></span><span style=display:flex><span>                         <span style=color:#e6db74>&#34;ring.number&#34;</span>,<span style=color:#e6db74>&#34;ring.type&#34;</span>,<span style=color:#e6db74>&#34;print&#34;</span>,<span style=color:#e6db74>&#34;population&#34;</span>,<span style=color:#e6db74>&#34;habitat&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#a6e22e>head</span>(mushrooms)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#75715e># Define the factor names for &#34;Class&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>levels</span>(mushrooms<span style=color:#f92672>$</span>Class) <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>c</span>(<span style=color:#e6db74>&#34;Edible&#34;</span>,<span style=color:#e6db74>&#34;Poisonous&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#75715e># Define the factor names for &#34;odor&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>levels</span>(mushrooms<span style=color:#f92672>$</span>odor) <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>c</span>(<span style=color:#e6db74>&#34;Almonds&#34;</span>,<span style=color:#e6db74>&#34;Anise&#34;</span>,<span style=color:#e6db74>&#34;Creosote&#34;</span>,<span style=color:#e6db74>&#34;Fishy&#34;</span>,<span style=color:#e6db74>&#34;Foul&#34;</span>,<span style=color:#e6db74>&#34;Musty&#34;</span>,<span style=color:#e6db74>&#34;None&#34;</span>,<span style=color:#e6db74>&#34;Pungent&#34;</span>,<span style=color:#e6db74>&#34;Spicy&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Define the factor names for &#34;print&#34;</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>levels</span>(mushrooms<span style=color:#f92672>$</span>print) <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>c</span>(<span style=color:#e6db74>&#34;Black&#34;</span>,<span style=color:#e6db74>&#34;Brown&#34;</span>,<span style=color:#e6db74>&#34;Buff&#34;</span>,<span style=color:#e6db74>&#34;Chocolate&#34;</span>,<span style=color:#e6db74>&#34;Green&#34;</span>,<span style=color:#e6db74>&#34;Orange&#34;</span>,<span style=color:#e6db74>&#34;Purple&#34;</span>,<span style=color:#e6db74>&#34;White&#34;</span>,<span style=color:#e6db74>&#34;Yellow&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#a6e22e>head</span>(mushrooms)
</span></span></code></pre></div><p>Now let&rsquo;s build our model. We are going to use <code>rpart</code> library to create the decision tree, and <code>rpart.plot</code> to visualize it.
But first, install <code>rpart.plot</code> if it&rsquo;s not already installed.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>install.packages</span>(<span style=color:#e6db74>&#34;rpart&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#a6e22e>install.packages</span>(<span style=color:#e6db74>&#34;rpart.plot&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#75715e># Import our required libraries</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>library</span>(rpart)
</span></span><span style=display:flex><span><span style=color:#a6e22e>library</span>(rpart.plot)
</span></span></code></pre></div><p>To create our decision tree model, we can use the <code>rpart</code> function. <code>rpart</code> is simple to use: you provide it a <code>formula</code>, show it the dataset it is supposed to use and choose a method (either &ldquo;class&rdquo; for classification or &ldquo;anova&rdquo; for regression).</p><p>A great trick to know when handling very large structured datasets (our dataset has over 20 columns we want to use!) is that in <code>formula</code> declarations, one can use the <code>.</code> operator as a quick way of designating &ldquo;all other columns&rdquo; to R. You can also <code>print</code> the Decision Tree model to retrieve a summary describing it.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#75715e># Create a classification decision tree using &#34;Class&#34; as the variable we want to predict and everything else as its predictors.</span>
</span></span><span style=display:flex><span>myDecisionTree <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>rpart</span>(Class <span style=color:#f92672>~</span> ., data <span style=color:#f92672>=</span> mushrooms, method <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;class&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># Print out a summary of our created model.</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>print</span>(myDecisionTree)
</span></span></code></pre></div><p>Now that we have our model, we can draw it to gain a better understanding of how it is classifying the data points. We can use the <code>rpart.plot</code>function &ndash; a specialized function for plotting trees &ndash; to render our model. This function takes on some parameters for visualizing the tree in different ways &ndash; try changing the type (from 1 to 4) parameter to see what happens!</p><p>If you run the code above, you&rsquo;ll see that our decision tree has perfect accuracy when classifying poisonous mushrooms, and almost perfect accuracy when dealing with edible ones.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>newCase  <span style=color:#f92672>&lt;-</span> mushrooms[10,<span style=color:#ae81ff>-1</span>]
</span></span><span style=display:flex><span>newCase
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>predict</span>(myDecisionTree, newCase, type <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;class&#34;</span>)
</span></span></code></pre></div><p><strong>Model accuracy :</strong></p><p>Let&rsquo;s split our dataset into traing set and test set :</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#75715e>## 75% of the sample size</span>
</span></span><span style=display:flex><span>n <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>nrow</span>(mushrooms)
</span></span><span style=display:flex><span>smp_size <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>floor</span>(<span style=color:#ae81ff>0.75</span> <span style=color:#f92672>*</span> n)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>## set the seed to make your partition reproductible</span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>set.seed</span>(<span style=color:#ae81ff>123</span>)
</span></span><span style=display:flex><span>train_ind <span style=color:#f92672>&lt;-</span> base<span style=color:#f92672>::</span><span style=color:#a6e22e>sample</span>(<span style=color:#a6e22e>c</span>(<span style=color:#ae81ff>1</span><span style=color:#f92672>:</span>n), size <span style=color:#f92672>=</span> smp_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>mushrooms_train <span style=color:#f92672>&lt;-</span> mushrooms[train_ind, ]
</span></span><span style=display:flex><span>mushrooms_test <span style=color:#f92672>&lt;-</span> mushrooms[<span style=color:#f92672>-</span>train_ind, ]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>newDT <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>rpart</span>(Class <span style=color:#f92672>~</span> ., data <span style=color:#f92672>=</span> mushrooms_train, method <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;class&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>result <span style=color:#f92672>&lt;-</span> <span style=color:#a6e22e>predict</span>(newDT, mushrooms_test[,<span style=color:#ae81ff>-1</span>], type <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;class&#34;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>head</span>(result)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span><span style=color:#a6e22e>head</span>(mushrooms_test<span style=color:#f92672>$</span>Class)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-r data-lang=r><span style=display:flex><span>base<span style=color:#f92672>::</span><span style=color:#a6e22e>table</span>(mushrooms_test<span style=color:#f92672>$</span>Class, result)
</span></span></code></pre></div><p>That&rsquo;s it! See you on another cool article :)</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Credit: <a href=https://www.linkedin.com/in/ehsanmkermani>Ehsan M. Kermani</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Credit: <a href="https://www.linkedin.com/in/saeedaghabozorgi/?originalSubdomain=ca">Saeed Aghabozorgi</a>, <a href=https://www.linkedin.com/in/walter-gomes/>Walter Gomes de Amorim Junior</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></div><div class="flex flex-row justify-around my-2"><h3 class="mb-1 mt-1 text-left mr-4"><a href=/blog/ml/ title="What is machine learning ?"><i class="nav-menu fas fa-chevron-circle-left"></i></a></h3><h3 class="mb-1 mt-1 text-left ml-4"><a href=/blog/supervised-algos-with-r-2/ title="Supervised Learning algorithms with R (part 2)"><i class="nav-menu fas fa-chevron-circle-right"></i></a></h3></div></main><footer class="text-sm text-center border-t border-gray-300 dark:border-gray-700 py-6"><p class=markdownify>Powered by <a href=https://gohugo.io/>Hugo</a> & deployed on <a href=https://pages.github.com/>GitHub Pages</a></p><p></p></footer></body></html>